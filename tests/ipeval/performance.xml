<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//OASIS//DTD DocBook XML V4.2//EN"
         "http://www.oasis-open.org/docbook/xml/4.2/docbookx.dtd" [
    <!ENTITY hyponymsSUMMARY SYSTEM "output/hyponyms/summary">
    <!ENTITY subclassesSUMMARY SYSTEM "output/subclasses/summary">
    <!ENTITY disambigSUMMARY SYSTEM "output/disambig/summary">
    <!ENTITY idisambigSUMMARY SYSTEM "output/disambig/summary">
    <!ENTITY getrefsSUMMARY SYSTEM "output/disambig/summary">
    <!ENTITY hyponymsCOMPARE SYSTEM "output/hyponyms/row.xml">
    <!ENTITY subclassesCOMPARE SYSTEM "output/subclasses/row.xml">
    <!ENTITY disambigCOMPARE SYSTEM "output/disambig/row.xml">
    <!ENTITY idisambigCOMPARE SYSTEM "output/disambig/row.xml">
    <!ENTITY getrefsCOMPARE SYSTEM "output/getrefs/row.xml">
    <!ENTITY hyponymsTASK "<emphasis role='bold'>HYPONYMS</emphasis>">
    <!ENTITY subclassesTASK "<emphasis role='bold'>SUBCLASSES</emphasis>">
    <!ENTITY disambigTASK "<emphasis role='bold'>DISAMBIG</emphasis>">
    <!ENTITY idisambigTASK "<emphasis role='bold'>IDISAMBIG</emphasis>">
    <!ENTITY getrefsTASK "<emphasis role='bold'>GETREFS</emphasis>">
]>
<article>
<articleinfo>
 <title>Performance of Iterated Partial Evaluation</title>
 <subtitle>Binding the Achilles Heel of Semantic Web Services</subtitle>
 <edition>Version 0.9</edition>
 <author>
  <firstname>Ken</firstname>
  <surname>Haase</surname>
  <affiliation>
   <orgname>beingmeta, inc.</orgname>
   <address><email>kh@beingmeta.com</email></address>
  </affiliation>
 </author>
 <copyright>
   <year>20050-2008</year>
   <holder role="mailto:kh@beingmeta.com">beingmeta, inc.</holder>
  </copyright>
 <abstract>
   <para>
   Iterated Partial Evaluation is a database optimization technique
   applicable to nearly arbitrary procedural queries and especially
   suited to composed queries with complex internal dependencies.  It
   works by repeatedly (often partially) reexecuting the logic of the
   query while delaying database accesses to be executed together, where
   the execution can proceed more efficiently.  This re-execution can
   replace or supplement the more limited static analysis used in more
   traditional database architectures.  This article describes the
   performance gains of this technique with respect to four different
   benchmark tasks.  The method is shown to provide speedups of over
   900% for certain queries and averages over 100% for some classes
   of queries (i.e. simple inheritance).
   tasks.
   </para>
  </abstract>
</articleinfo>

<sect1 id='Introduction'><title>Introduction</title>

<para>This article documents the performance gains provided by a
database query optimization technique called <emphasis>iterated
partial evaluation</emphasis>.  Iterated partial evaluation is
especially designed for database queries with complex internal
dependencies applied against large distributed databases.  These are
especially common in knowledge-based processing, where processing is
heavily data directed and involves chains of pointer following in
ontologies or other knowledge structures.  They are also one of the
primary characteristics of "Semantic Web" computing, a growing
paradigm which combines knowledge-based processing with distributed
resources.</para>

<para>The optimization technique applies to nearly arbitrary
procedural queries because it does not rely on the formal analysis of
the query itself for optimization.  Instead, it repeatedly executes
the query in a manner which allows parts of the query to fail while
other parts proceed.  These repeated executions refer to an in-memory
cache of data fetched from external sources.  During the course of
query execution, whenever some external data is needed, the need is
noted and that part of the execution fails.  When the incomplete
execution is finished, the needed data is requested together and the
results stored in the in-memory cache.  The query is then reexecuted
and the cycle continues until all of the data needed by the query is
cached and it can complete without failure.</para>

<para>The technique yields performance improvements for two reasons.
First, processor performance has improved exponentially (following
Moore's law) for decades while I/O performance has generally improved
more slowly.  This means that it may often be worth reexecuting the
logic of a query if it can improve aggregate I/O performance.  Second,
the bundling together of data access operations, in between execution
cycles, can in fact improve aggregate I/O performance.</para>

<para>There are, in turn, two particular reasons that bundling
together access operations can improve performance.  The first is that
aggregation allows optimization of accesses to physical media, for
example by ordering disk accesses to reflect disk geometries or
improve cache coherence.  Second, and more significantly, while raw
I/O bandwidth has also increased exponentially, I/O latency has
generaly improved only linearly.  This means that there are
substantial advantages to bundling a request for multiple data
elements into a single transaction.  This is especially the case for
network based databases, where the latency of network response can be
substantial.  This kind of complex integration of networked data
sources is the paradigm for Semantic Web applications and many
envisioned web service applications.  We argue below <xref
linkend='Achilles'/> that network latency is a largely unrecognized
"Achilles Heel" for next-generation web services and the Semantic Web
in general.</para>

<para>Iterated re-execution of queries identifies dependencies and can
either replace or supplement the static analysis of queries employed
by more conventional database architectures.  Iterated re-execution is
more general in that it can yield improvements in queries which are
resistant or opaque to formal analysis or where potential
optimizations emerge dynamically from the data itself.</para>

<para>beingmeta has filed for a patent on this method while continuing
to explore refinements, mostly based on further internal caching to
lessen the cost of the repeated executions.  In addition, there may be
other refinements that could make use of multi-way SMP machines, new
multi-core processors, and asynchronous disk I/O.</para>

<sect2><title>Summary of Results</title>

<para>We evaluated the method on four different benchmark tasks
operating over large ontologies and data sets provided by a
network-based knowledge server.  Two of the tasks were directly
modelled on tasks drawn from knowledge-based applications developed by
beingmeta.  Application of the method to the direct execution of query
tasks yielded average performance improvements ranging from 15% to
120% with maximum improvements of nearly 900%.  In the one task where
performance improvements were less substantial, applying the technique
to parts of the query procedure, rather than the entire procedure,
yielded improvements averaging 220%.  Finally, the degree of
improvement is seen to trend higher as queries grow more large (in
terms of data requried) and more complex (in terms of internal
dependencies).</para> </sect2> </sect1>

<sect1 id='BenchmarksSECT'><title>The Benchmarks</title>

<para>Iterated partial evaluation was developed for optimizing search
and inference in knowledge-based applications relying on large
background databases.  Queries in such applications often involve
chains of dependent retrievals and calculations which implement
patterns such as pointer following, taxonomic inheritance, or
instance-based matching.  To evaluate the method, we have taken four
different tasks to evaluate the technique.  Two are straightforward
inheritance/pointer following techniques and two are natural language
processing tasks which use an external knowledge base to resolve
ambiguous references.</para>

<sect2><title>The Hardware</title>

<para>These benchmarks were executed on standard hardware running the
Ubuntu distribution of Debian Linux with version 2.6 of the Linux
kernel.  Two machines were used, with one machine executing the
queries themselves and one serving as the database server.  The query
machine was a single-processor Pentium 4 running at 2.4GHZ with a
512KB cache and 512MB of physical memory.  The database server was a
dual-processor 2GHz Athlon processor with a 256KB cache and 2GB of
physical memory.  This machine was chosen as the database server to
reduce server-side factors in the timings, since its main memory could
readily hold all of the information used for any particular
query.</para>

<para>The client and server were both substantially unloaded and
connected to one another with conventional 100MBPS Ethernet.
Effective network latency roughly averaged 160 microseconds, as
measured by the Unix <literal>ping</literal> diagnostic's RTT (round
trip time).</para>

</sect2>

<sect2><title>The Software</title>

<para>The method of iterated partial evaluation was orginally
developed in the context of FramerD <citation><xref
linkend='FramerD'/></citation>, an open source object database
optimized for pointer-intensive operations.  FramerD makes extensive
use of caches at multiple levels to improve performance and supports
databases in both disk files and network servers.</para>

<para>For this evaluation, the query client and database server both
used a new proprietary implementation of FramerD with improved support
for iterated partial evaluation and easier access for instrumentation.
In order to simplify benchmarking, we attempt to normalize for
variations at the operating system level.  In particular, we run each
query to completion once before doing the comparative timings.  This
ensures that the remote server internally caches the neccessary data
and responds consistently to client requests.</para>

<para>FramerD provides two basic data storage models.
<firstterm>Object references</firstterm> resolve 64-bit object IDs to
complex structured objects.  <firstterm>Index references</firstterm>
resolve arbitrary complex structured objects to collections of other
objects, typically object references.  The complex structured objects
include all the common objects (numbers, strings, vectors, lists, etc)
provided by most dynamically typed languages.  In addition, they
include object references, unordered collections of objects (result
sets or <emphasis>choices</emphasis>) and
<emphasis>slotmaps</emphasis> which are tables mapping keys to one
another.</para>

<para>The storage models provide a basic implementation layer for
frame representations <citation><xref
linkend='MinskyFrames'/></citation> and the semantic networks they
constitute.  In a frame representation, we take a
<firstterm>frame</firstterm> to be a collection of
<firstterm>slots</firstterm> each uniquely identified by a
<firstterm>slotid</firstterm> and referring to a collection of
constant objects or object references (the slot's
<firstterm>values</firstterm>).  The frame representation can be
reified as an entity-relationship model or as a conventional
triple-store, but we find the frame model to be more cognitively
perspicuous.</para>

<para>The frame representation layer uses the implementation layer in
two particular ways. First, a frame is an object reference resolving
to a slotmap, so we can retrieve a slot from a frame by resolving it
to a slotmap and looking up the corresponding
<firstterm>slotid</firstterm> on the slotmap.  Second, searches for
frames can be executed through index references which map slotid/value
pairs to object references possessed of those slots.  These pairs are
simply compound objects consisting of the slotid and the requested
value.</para>

<para>Above the frame representation layer, there is an
<emphasis>inference layer</emphasis> implemented by the use of slotids
which are themselves frames.  At this layer, these slot-frames have
slots specifying methods which can be used to compute or test for
values.  Such <emphasis>virtual slots</emphasis> can perform involved
calculations, including structural traversal or database search, in
order to return inferred values.  For example, certain relations (like
subclasses and superclasses) are natural inverses, and retrieving one
of these relations automatically invokes a database search for the
inverse relationship as well as using locally asserted connections.</para>

<para>Though not used in these examples, there are also methods for
asserting and retracting these virtual slots which allow for
consistency with the methods for getting and testing them.  The
methods provided by the inference layer were originally based on the
CycL language <citation><xref linkend='CYCL'/></citation>
but generally align with the facilities of <emphasis>description
logics</emphasis>.</para>

<para>The inference layer provides caching of virtual slot values and
tests to avoid needless recomputation.  This caching is made sensitive
to iterated partial evaluation (where computing a slot may fail
because it needs external resources), by only caching a computed value
when the computation of the value or test did not require any uncached
data accesses.  In these benchmarking results, of course, these caches
are cleared between trials.</para>

<para>The version of FramerD used in these trials provides a simple
Scheme-based scripting language, which was used to code the benchmark
tasks.  This was chosen for simplicity over the bindings to Python or
other scripting languages.</para>

<para>FramerD was developed to allow the integration of
knowledge-based processing into a range of applications and to provide
efficient access to very large knowledge bases.  These knowledge bases
are used in the benchmarks we discuss and they are detailed
below.</para>

</sect2>

<sect2><title>The Databases</title>

<para>All of the benchmarks make use of two large knowledge bases as
the background to their queries.  BRICO <citation><xref
linkend='BRICO'/></citation> is an interlingual ontology originally
developed at MIT by hybridizing WordNet <citation><xref
linkend='WordNet'/></citation> with other
lexical resources and linking the concepts into multiple languages
using a variety of heuristic methods audited (in some cases) by human
experts.  XBRICO is beingmeta's proprietary extension of BRICO that
includes concepts derived from text mining compound nouns and
individual names and places derived from both text mining and public
information sources.  BRICO itelf consists of roughly 250,000 frames,
while XBRICO consists of roughly 140,000 frames for concepts, 46,000
frames for individuals and 4,000,000 frames for a wide range of
geographical locations.  Together, this totals roughly 4.5 million
frames.</para>

<para>One of the benchmark tasks makes use of a large annotated image
corpus which is part of the BabelVision <citation><xref
linkend='BabelVision'/></citation> image database system.  The images
are annotated with concepts from BRICO/XBRICO which were produced by a
semi-automatic mapping of controlled vocabulary keywords into the
ontology.  The corpus contains descriptions of 75,000 images, each
annotated with between a half dozen and two dozen concepts.</para>

</sect2>

<sect2><title>The Tasks</title>

<para>We chose four different benchmarks to run our comparisons.  They
are:</para>
<itemizedlist>

<listitem><para> &hyponymsTASK; (simple hierarchy descent): The
<literal>HYPONYM</literal> relationship is a relationship between
WordNet synsets as reified in BRICO.  The &hyponymsTASK; task retrieves the
transitive closure of this relationship for a particular node.  This
basically tests the algorithm in a case where the computation is
relatively simple and the dependencies relatively straightforward.
</para></listitem>

<listitem><para> &subclassesTASK; (inferred hierarchy descent): The
<literal>SUBCLASSES</literal> relationship is a BRICO relationship
which is computed in two ways: direct access to data level slots (such
as <literal>HYPONYM</literal> from WordNet <citation><xref
linkend='WordNet'/></citation> or
<literal>ROGET-WITHIN</literal> from the 1911 Roget's thesaurus) and
indexed search for the inverse <literal>SUPERCLASSES</literal>
relationship.  The &subclassesTASK; task retrieves the transitive
closure of the <literal>SUBCLASSES</literal> relationship.  This
evaluates the method with a computationally more intensive task that
involves more complex internal dependencies.  </para></listitem>

<listitem><para> &disambigTASK; (corpus based disambiguation): This
task uses an annotated corpus to disambiguate a keyword query into
concepts <citation><xref linkend='ContextDisambig'/></citation>.  The
task works by constructing two tables of rankings.  The first table
scores content items in the corpus based on their association with
possible meanings of words in a keyword query.  The second table ranks
possible meanings based on the sum of the scores of content items they
annotate.  This method has been shown to be highly reliable in
disambiguating keyword queries into concepts.  The task is quite
computationally intensive and involves the creation of substantial
internal state, stressing an algorithm which relies on repeated
executions of the query logic.  &idisambigTASK; is a modified version
of the &disambigTASK; task which applies the method to parts of the
query rather than the whole query.</para> </listitem>

<!--
<listitem><para>&getrefsTASK; (contextual reference analysis): This task takes a
document which has been broken into paragraphs and had named extracted
using simple heuristics (mostly capitalization cues).  It then
attempts to disambiguate these references to an external database in
two passes.  The first pass looks for unambiguous referents based on
lexical identity <emphasis>a priori</emphasis> without any contextual
information.  The second pass combines lexical identity with the
locational context information derived from the first pass.  For
instance, the name "Maine" is an unambiguous place name while
"Portland" is ambiguous place name.  As the algorithm proceeds,
"Maine" would be resolved in the first pass but Portland would not.
However, in the second pass, the context of "Maine" would allow the
disambiguation of the reference to "Portland" to identify the east
coast city.  </para></listitem>
-->
</itemizedlist>

<para>The &hyponymsTASK; and &subclassesTASK; tasks measure the method's
performance on the simplest kind of query but distinguishes the case
of simple retrieval (&hyponymsTASK;) and inferential retrieval (&subclassesTASK;).
<!--
The &disambigTASK; and &getrefsTASK; tasks both involve complex query logic and
substantial computation in the execution phase of the method.
-->
The &disambigTASK; task involves complex query logic and
substantial computation in the execution phase of the method.
</para>

<para>For each of these tasks, we compared the tasks over a set of
randomly selected inputs.  For each input, we evaluated the query
eight times.  First, we evaluated it once without timing to reduce
error or bias from priming or caching effects.  Then, we executed it
three times without the method and three times with the method,
averaging the three runs.  Finally, we executed it once with an
instrumented version of the method which provided number of cycles,
database references, and time spent in fetching references or
executing query logic.  The time taken by the final cycle of this
instrumented execution, where all the database references were cached,
is taken as the "query logic time" and capture the tasks' core
computational complexity.</para>

<para>The following table summarizes the four tasks in terms of
unassisted execution time, number of object references, and query
logic execution time.  The latter number was derived from the final
cycle of the assisted method, when all database references had been
cached.</para>

<table><title>Task Characteristics</title>
<tgroup cols='11'>
<spanspec spanname='time' namest='meantime' nameend='maxtime'/>
<spanspec spanname='refs' namest='meanrefs' nameend='maxrefs'/>
<spanspec spanname='qlt' namest='meanqlt' nameend='maxqlt'/>
<thead>
  <colspec colnum='3' colname='meantime'/>
  <colspec colnum='4' colname='mintime'/>
  <colspec colnum='5' colname='maxtime'/>
  <colspec colnum='6' colname='meanrefs'/>
  <colspec colnum='7' colname='minrefs'/>
  <colspec colnum='8' colname='maxrefs'/>
  <colspec colnum='9' colname='meanqlt'/>
  <colspec colnum='10' colname='minqlt'/>
  <colspec colnum='11' colname='maxqlt'/>
  <row>
   <entry>Task</entry><entry># of Trials</entry>
   <entry spanname='time'>execution time</entry>
   <entry spanname='refs'># of database accesses</entry>
   <entry spanname='qlt'>query logic execution time</entry>
  </row>
  <row>
   <entry morerows='2'></entry>
   <entry>mean</entry><entry>min</entry><entry>max</entry>
   <entry>mean</entry><entry>min</entry><entry>max</entry>
   <entry>mean</entry><entry>min</entry><entry>max</entry>
  </row>
</thead>
<tbody>
  &hyponymsSUMMARY;
  &subclassesSUMMARY;
  &disambigSUMMARY;
</tbody>
</tgroup>
</table>

</sect2>
<sect2><title>Summary of Task Performance</title>

<para>The tables below summarize results for each of the above tasks.
The data provide both a general characterization of the task
(execution time, number database references, query logic time) and of
the improvement gained by applying the method to it (optimized
execution time and average percent speedup).</para>

<para>For characterizing the task itself, the "# of database accesses"
and "query logic time" capture the degree to which the task emphasizes
data access or processing.  Both of these are derived from
instrumentation on the iterated partial evaluation process: the first
is a cumulative count of delayed object references and the second is
the execution time for the final cycle of the method, which indicates
exeuction time when all required data are cached in local
memory.</para>

<para>For looking at the performance of the method, the execution
times and the "speedup" are the important guides.  Note that the
speedup value is the average of the speedup per query, which is why it
is not a simple function of the modified and unmodified query
execution times, since the degree of speedup varies with respect to
the actual execution times.</para>

<table><title>Comparison of the method across tasks</title>
<tgroup cols='7'>
<spanspec spanname='speedup' namest='meanspeedup' nameend='maxspeedup'/>
<thead>
  <colspec colnum='3' colname='meanspeedup'/>
  <colspec colnum='4' colname='minspeedup'/>
  <colspec colnum='5' colname='maxspeedup'/>
  <row>
   <entry>Task</entry><entry># of Trials</entry>
   <entry spanname='speedup'>speedup%</entry>
   <entry>exec time wo/method</entry>
   <entry>exec time w/method</entry>
   <entry># of database accesses</entry>
   <entry>query logic time</entry>
  </row>
</thead>
<tbody>
  &hyponymsCOMPARE;
  &subclassesCOMPARE;
  &disambigCOMPARE;
</tbody>
</tgroup>
</table>

</sect2>

</sect1>

<sect1 id='HYPONYMSsect'><title>The &hyponymsTASK; task</title>

<para>The &hyponymsTASK; query is simply the Scheme procedure:
<programlisting>
(define (hyponyms x)
  (choice x (hyponyms (get x 'hyponym)))
</programlisting>
Where the <literal>GET</literal> procedure does a simple slot
retrieval and the <literal>CHOICE</literal> procedure invokes
FramerD's <emphasis>non-determinstic evaluator</emphasis>.  The choice
procedure returns its arguments as an intermediate result set.
Normally, when a procedure is called on such a result set, it
automatically iterates over the elements of the result set and
combines their results.  FramerD has substantial optimizations for the
generation and handling of these result sets which are sometimes called
<emphasis>choices</emphasis>.</para>

<para>Using the query procedure above, we compared the performance of
iterated partial evaluation with straight-line execution over 1000
samples selected at random from the subset of BRICO and XBRICO which
had any hyponyms at all.  On average, queries without the method took
16.8 milliseconds, while queries using the method averaged 4.1
milliseconds.  Overall,the tasks averaged 17 database accesses per
query and the query logic time averaged 148 microseconds.  This
reflects the computational simplicity of the query.  The average time
spent resolving database references was 3.34 milliseconds with the
method and 16.7 milliseconds (estimated) without the method.  This
shows the advantage to query aggregation.  Finally, when the method
was applied, it repeated partially executed the core query an average
of 3.63 times across the sample inputs.</para>

<para>The method improved performance on the &hyponymsTASK; task by an
average 114% and ranged up to 895% on certain queries.  However, in
332 of the cases, the method slightly slowed query execution,
averaging -3.4% with a worst case of -41%.  In all of these cases,
only two or three database references occured, so the bundling of
these references together did not yield enough advantage to compensate
for the cost of repeated executions.  Significantly, when we only
consider queries making more than three database references (496 of
them), the average performance improvement was actually 218%.</para>

<para>The graphs below show comparative performance against several
query specific parameters, including total number of database
references, query execution time without any external references
(query logic alone), and total time spent performing external database
references by the method.  For this task, all of these metrics
correlated roughly with the size of the subtree enumerated by the
query.</para>

<figure><title>Comparison on HYPONYM against database references</title>
 <mediaobject>
  <imageobject>
    <imagedata fileref="hyponyms/dbrefs.png" format="PNG" scale="33"/>
  </imageobject>
  <imageobject>
   <imagedata fileref="hyponyms/dbrefs" format="EPS" scale="33"/>
  </imageobject>
 </mediaobject>
</figure>
<para>Clearly, as the number of database references rises, the advantage
of bundling references increases.
</para>

<figure><title>Comparison on HYPONYM against the query logic time</title>
 <mediaobject>
  <imageobject>
    <imagedata fileref="hyponyms/exectime.png" format="PNG" scale="33"/>
  </imageobject>
  <imageobject>
   <imagedata fileref="hyponyms/exectime" format="EPS" scale="33"/>
  </imageobject>
 </mediaobject>
</figure>

<para>This graph shows comparative performance against the query logic
time: the execution time of a complete query all of whose database
references are cached.  This corresponds to the final stage of an
iterated partial evaluation and captures the time required by the
query logic alone.  In this case, this is also closely related to
subtree size and the trend clearly shows that the improvement
increases for larger problems.</para>

<para>Finally, the following graph compares performance against the
total time spent fetching references during query execution under the
method.  This is distinct from the total number of database references
because the fetch time is based on the bundling together of operations
by which the method gains its advantage.</para>
<figure><title>Comparison on HYPONYM against fetch time</title>
 <mediaobject>
  <imageobject>
    <imagedata fileref="hyponyms/fetchtime.png" format="PNG" scale="33"/>
  </imageobject>
  <imageobject>
   <imagedata fileref="hyponyms/fetchtime" format="EPS" scale="33"/>
  </imageobject>
 </mediaobject>
</figure>

<para>Clearly, performance using the method is substantially improved
and the gain grows as problem size increases along a variety of
metrics.</para>

<sect2><title>The Lifecycle of a particular query</title>

<para>The following diagram shows the progress of a particular query
in the system.  Time is shown along the horizontal axis and database
accesses are represented by the boxes whose height indicates the
number of database referenced performed.
<figure><title>Execution timeline for a &hyponymsTASK; task</title>
 <mediaobject>
  <imageobject>
    <imagedata fileref="hyponyms/timeline.png" format="PNG" scale="33"/>
  </imageobject>
  <imageobject>
   <imagedata fileref="hyponyms/timeline" format="EPS" scale="33"/>
  </imageobject>
 </mediaobject>
</figure>
This query makes 75 database references in all and requires 5
iterations to complete.  Thus, the execution only takes 5 (albeit
large) database references.  This timing diagram also shows how the
execution time (the space between the bars) increases in later cycles.
(Note that the first execution cycle is not visible because it is only
26 microseconds long, too short to show at this scale).
</para>

</sect2>
</sect1>

<sect1 id='SUBCLASSESsect'><title>The &subclassesTASK; task</title>

<para>The &subclassesTASK; task is more computationally intensive than
the &hyponymsTASK; task because it descends a tree virtually computed
by FramerD's automatic inferences.  The &subclassesTASK; relationship
is defined (by methods stored in the FramerD database) as a virtual
slot which combines the values of various other slots (including
&hyponymsTASK; and ROGET-WITHIN (for concepts from Roget's thesaurus))
as well as searching for objects with an inverse
<literal>SUPERCLASSES</literal> relationshp referring to the object.
The difference between the two methods involves both extended
computation and more database references (for the search operation).
Note that the accesses to the multiple slots of the same frame only
require one database reference to retrieve the frame's slotmap.</para>

<para>The query in this case is implemented by a similarly simple
Scheme procedure to &hyponymsTASK;.
<programlisting>
(define (get-subclasses x)
   (choice x (get-subclasses (get x @?subclasses)))
</programlisting>
The syntax <literal>@?subclasses</literal> refers to the frame
uniquely identified by the term <literal>subclasses</literal>.  This
reference is resolved at load/read time and so does not factor into
the query itself.  The slot description itself (which is loaded as
part of the query), has the following frame properties:
<programlisting>
;; GET-METHODS:		{FD:MULTI-GET FD:INV-GET}
;; TEST-METHODS:	{FD:MULTI-TEST FD:INV-TEST}
;; INVERSE:		@/brico/2c272"SubClassOf"
;; SLOTS:		{HYPONYM ROGET-CONTAINS}
;; OBJ-NAME:		"SubClasses"
;; MNEMONIC:		{SUBCLASSES SPECLS}
</programlisting>
</para>

<para>Using the query procedure above, we compared the performance of
iterated partial evaluation with straight-line execution over the same
1000 samples for the &hyponymsTASK; task.  On average, unassisted queries
took 21 milliseconds, while assisted queries took 7.2
milliseconds.</para>

<para>The queries averaged 140 database accesses per query and the
average query logic time averaged 376 microseconds.  In contrast to 17
database accesses and 148 microseconds for the &hyponymsTASK; task,
this reflects the greater computational load of a task which included
making inferences as well as fetching data.  Most of these accesses
were index references attempting to find inverse
<literal>SUPERCLASSES</literal> relationships, and most of these
yielded little fruit.  This is why the average query logic time
increased by roughly only 250% even though database references
increased by over 800%.  The average time spent resolving database
references was 4.97 milliseconds with the method and 20.6 milliseconds
(estimated) without the method.  Across the sample inputs, application
of the method averaged 3.63 execution cycles per query.</para>

<para>The method's performance advantage on the &subclassesTASK; task
averaged 88% and ranged to over 400% (429%) in some
cases.  Of the 1000 queries, performance declined in only 4 cases but
did so more substantially than in the &hyponymsTASK; task, averaging -25%
with a worst case of -50%.</para>

<para>As with &hyponymsTASK;, the following graphs compare the performance
of iterated partial evaluation with straight-line execution over the
same 1000 samples selected at random from the subset of BRICO and
XBRICO which had any hyponyms at all.</para>

<figure><title>Comparison on &subclassesTASK; against database references</title>
 <mediaobject>
  <imageobject>
    <imagedata fileref="subclasses/dbrefs.png" format="PNG" scale="33"/>
  </imageobject>
  <imageobject>
   <imagedata fileref="subclasses/dbrefs" format="EPS" scale="33"/>
  </imageobject>
 </mediaobject>
</figure>

<figure><title>Comparison on &subclassesTASK; against the query logic time</title>
 <mediaobject>
  <imageobject>
    <imagedata fileref="subclasses/exectime.png" format="PNG" scale="33"/>
  </imageobject>
  <imageobject>
   <imagedata fileref="subclasses/exectime" format="EPS" scale="33"/>
  </imageobject>
 </mediaobject>
</figure>

<figure><title>Comparison on &subclassesTASK; against fetch time</title>
 <mediaobject>
  <imageobject>
    <imagedata fileref="subclasses/fetchtime.png" format="PNG" scale="33"/>
  </imageobject>
  <imageobject>
   <imagedata fileref="subclasses/fetchtime" format="EPS" scale="33"/>
  </imageobject>
 </mediaobject>
</figure>

<sect2><title>The Lifecycle of a particular query</title>

<para>The following diagram shows the progress of a particular query
in the system.  Time is shown along the horizontal axis and database
accesses are represented by the boxes whose height indicates the
number of database reference performed.
<figure><title>Execution timeline for a &subclassesTASK; task</title>
 <mediaobject>
  <imageobject>
    <imagedata fileref="subclasses/timeline.png" format="PNG" scale="33"/>
  </imageobject>
  <imageobject>
   <imagedata fileref="subclasses/timeline" format="EPS" scale="33"/>
  </imageobject>
 </mediaobject>
</figure>

This timeline shows both the larger number of database references used
in this query, reflecting inferences which look for other
relationships (in particular, <literal>SUPERCLASSES</literal>) which
imply a subclasses relationship.  The greater gap between the fetches
also reflects the increased computation involved in combining results.
</para> </sect2> </sect1>

<sect1 id='DISAMBIGsect'><title>The &disambigTASK; task</title>

<para>The &disambigTASK; task uses an annotated corpus to disambiguate
keyword terms into unambiguous concepts.  It is based on the intuition
that when a keyword might refer to several concepts, the frequency of
each concept within the annotated corpus may reflect the likelihood of
the keyword's actual meaning.  The method extends this intuition to
consider sets of keywords, looking at conditional frequencies of
meanings given the other possible meanings in the query.</para>

<sect2><title>The query logic</title>

<para>The query itself is computationally involved, dynamically
constructing two tables assigning numeric scores to particular frames
and using those scores in computing the most likely meaning of each
keyword term.  The first table <emphasis>scores images</emphasis> in
the database based on their possible relevance to the query.  The
second table <emphasis>scores meanings</emphasis> based on the
relevance score of the images they are associated with.  This allows
the context of the query terms to combine with the background of
annotated images to disambiguate term meaning.</para>

<para>The first table is generated by retrieving images based on all
the possible meanings of the terms in the query and scoring those
images based on how many of the query terms find it.  This calculation
uses the structure of the knowledge base by expanding along the
subclasses relationship in finding images which might match the query
terms.
<programlisting>
;;; This computes scores across the image database given all the
;;; possible meanings of all the keywords in the query.  It does so by
;;; giving each image one point for matching a possible meaning of one
;;; of the keywords.
(define (get-corpus-scores keylist index) 
  (let* ((corpus-scores (make-hashtable 4096))) 
    (dolist (key keylist)
      ;; Search BRICO/XBRICO for meanings
      (do-choices (meaning (find-frames background @?english key))
        ;; For each meaning, increment the score of images annotated
        ;;  with it or its subclasses.
	(hashtable-increment!
	  corpus-scores (find-frames index
	                  @?gn/concepts
                          (get meaning @?subclasses*)))))
    corpus-scores))
</programlisting>
</para>

<para><emphasis role="strong">Reading the code.</emphasis> The
<literal>do-choices</literal> form iterates over the elements of a
result set.  The <literal>@?gn/concepts</literal> slotid is where the
concepts associated with an image are stored.  Values for this slotid
are simply stored without any computed inference.  The
<literal>@?subclasses*</literal> slotid is a virtual slot which does a
recursive descent through the subclasses relationship (just as we
computed in the &subclassesTASK; task).  The
<literal>find-frames</literal> function use one particular index to
identify frames with particular slot/value relationships.  The
<literal>background</literal> variable specifies indices to the BRICO
and XBRICO ontologies.  In the procedure above,
<literal>find-frames</literal> is used to find all the concepts that a
particular english term might refer to and to find all the images in
the database that have certain concepts associated with them.</para>

<para>The scores assigned to the corpus are then used to assign scores
to individual meanings:
<programlisting>
;;; This computes scores for every possible meaning of the keywords in
;;; the keylist.  It does this by taking the corpus scores and adding
;;; together the corpus scores for all images
(define (get-meaning-scores keylist index)
  (let* ((corpus-scores (get-corpus-scores keylist index))
	 (meaning-scores (make-hashtable 1024)))
    (dolist (key keylist)
      ;; Get all possible meanings from BRICO/XBRICO
      (do-choices (meaning (find-frames background @?english key))
        ;; Score each meaning by adding together the relevance scores
        ;;  of the associated concepts.
	(do-choices (instance (find-frames index
				gn/concepts (get meaning @?subclasses*)))
	  (hashtable-increment! meaning-scores meaning
				(get corpus-scores instance)))))
    meaning-scores))
</programlisting>
<emphasis role="strong">Reading the code.</emphasis>
The <literal>hashtable-increment!</literal> procedure increments the
value stored under a particular key in the table.  The table returned
by this procedure associates an integral score with each of the
possible meanings of the terms in <literal>keylist</literal>.
</para>

<para>Given the results of the above procedures, picking the best
meaning is simply a matter of using the <literal>largest</literal>
primitive on the possible meanings:
<programlisting>
(define (disambig keylist)
  (let ((scores (get-meaning-scores keylist image-index)))
    (map (lambda (entry)
	   (cons entry
	     (largest (find-frames background @?english entry)
	              scores)))
	 keylist)))
</programlisting>
</para>
</sect2>

<sect2><title>Task Performance</title>

<para>Because it was more computationally intensive, we reduced the
number of trials for this task, executing 200 comparative trials of
the query procedure using sample keylists synthesized from the
database itself.  The keylists varied in length from two to five terms
and were chosen by selecting random images, selecting random concepts
describing the image, and selecting random words describing the
concept.  In this task, queries without the method averaged 1.8
seconds while queries with the method ran slower, taking 1.88 seconds.
The average number of database references per query was 11,705 and the
average query logic execution time (no database references) was 267
milliseconds.  Without the method, database accesses used an
(estimated) 1.543 seconds.  With the method, the time spent on
database accesses averaged 121 milliseconds.</para>

<para>Despite the increase in average query time with the method,
performance improvement overall was mildly positive, averaging a
modest 15.5% and peaking at nearly 600%.  This modest improvement
indicates the cost of repeatedly executing the complex query logic
used by the procedure.  Indeed, well over half (122) of the queries
suffered from reduced performance, averaging -33% with a worst case of
-63%.  We can again look at the comparative performance graphed
against various query specific parameters.  This task shows much less
consistent behavior than the previous tasks, mostly because the query
logic is more complex and the task complexity is not a direct function
of the number of objects referenced.</para>

<figure><title>Comparison on &disambigTASK; against database references</title>
 <mediaobject>
  <imageobject>
    <imagedata fileref="disambig/dbrefs.png" format="PNG" scale="33"/>
  </imageobject>
  <imageobject>
   <imagedata fileref="disambig/dbrefs" format="EPS" scale="33"/>
  </imageobject>
 </mediaobject>
</figure>

<figure><title>Comparison on &disambigTASK; against query logic time</title>
 <mediaobject>
  <imageobject>
    <imagedata fileref="disambig/exectime.png" format="PNG" scale="33"/>
  </imageobject>
  <imageobject>
   <imagedata fileref="disambig/exectime" format="EPS" scale="33"/>
  </imageobject>
 </mediaobject>
</figure>

<figure><title>Comparison on &disambigTASK; against fetch time</title>
 <mediaobject>
  <imageobject>
    <imagedata fileref="disambig/fetchtime.png" format="PNG" scale="33"/>
  </imageobject>
  <imageobject>
   <imagedata fileref="disambig/fetchtime" format="EPS" scale="33"/>
  </imageobject>
 </mediaobject>
</figure>

</sect2>

<sect2><title>The Lifecycle of a particular query</title>

<para>The following diagram shows the progress of a particular query
in the system.  Time is shown along the horizontal axis and database
accesses are represented by the boxes whose height indicates the
number of database referenced performed.  Because of the number of
database accesses involved, in this task, the vertical access is
log-scaled.
<figure><title>Execution timeline for a &disambigTASK; task</title>
 <mediaobject>
  <imageobject>
    <imagedata fileref="disambig/timeline.png" format="PNG" scale="33"/>
  </imageobject>
  <imageobject>
   <imagedata fileref="disambig/timeline" format="EPS" scale="33"/>
  </imageobject>
 </mediaobject>
</figure>

The timeline here clearly shows the increased query execution time
(the spaces between the blocks) which leads to the reduced advantage
to the method.  However, the blocks of fetches are quite large,
suggesting a potential advantage to be exploted.  The next section
describes a way of doing this.
</para>

</sect2>

<!--
<sect2><title>&disambigTASK; revisited</title>

<para>The failure of iterated partial evaluation to improve the
performance on this task is largely due to the cost of each query
execution cycle.  We can address this be rewriting the query so that
the method is applied to components of the query where the most
advantage can be gained and the results of those components combined
only once in a single execution.  We can use the
<literal>IPEVAL</literal> primitive in the
<literal>get-meaning-scores</literal> to apply the method selectively
to the computation of the corpus map and other searches, as follows:

<programlisting>
(define (get-meaning-scores keylist index)
  (let* ((corpus-scores <emphasis role='strong'>(ipeval (get-corpus-scores keylist index))</emphasis>)
	 (meaning-scores (make-hashtable 1024))
	 (n-meanings 0))
    (dolist (keyword keylist)
      (do-choices (meaning (?? @?english keyword))
	(do-choices (instance
		     (ipeval (find-frames index
		                 gn/concepts (get meaning @?specls*))))
	  (hashtable-increment!
	     meaning-scores meaning
             (get corpus-scores instance)))))
    meaning-scores))
</programlisting>
</para>

<para>When we compare straight-line execution of the procedure to
execution of this modified procedure, we find a dramatic improvement.
Queries without the method averaged 1.83 seconds, while queries using
the method averaged 0.52 seconds.  The average speedup is now 214%
with a best case speedup of over 650%.  With the
modified procedure, only 6 of the 200 queries suffered from reduced
performance and only by -6%.  The performance comparison
graphs now look more consistent with the &hyponymsTASK; and &subclassesTASK;
tasks.  (We don't show the final execution time, since it can be
difficult to track when there are multiple calls to IPEVAL.</para>

<figure><title>Comparison on modified &idisambigTASK; against database references</title>
 <mediaobject>
  <imageobject>
    <imagedata fileref="idisambig/dbrefs.png" format="PNG" scale="33"/>
  </imageobject>
  <imageobject>
   <imagedata fileref="idisambig/dbrefs" format="EPS" scale="33"/>
  </imageobject>
 </mediaobject>
</figure>

<figure><title>Comparison on modified &idisambigTASK; against query logic time</title>
 <mediaobject>
  <imageobject>
    <imagedata fileref="idisambig/exectime.png" format="PNG" scale="33"/>
  </imageobject>
  <imageobject>
   <imagedata fileref="idisambig/exectime" format="EPS" scale="33"/>
  </imageobject>
 </mediaobject>
</figure>

</sect2>
-->

</sect1>

<!--
<sect1 id='GETREFSsect'><title>The &getrefsTASK; task</title>

<para>The &getrefsTASK; task is a different kind of compute-intensive query,
this time focused on the subset of the BRICO/XBRICO knowledge bases
describing individuals.  It is based on the idea of analyzing
ambiguous document terms by using document context to constrain the
search for meanings.</para>

<sect2><title>The query logic</title>

<para>The query logic for this task is naturally divided into two
phases.  The first phase determines all of the unambigous references
in the document and its passages.  In this case, the order of analysis
is irrelevant.  The second phase proceeds through the passages in
order, using the context to constrain the search for concepts
associated with references.  When references are resolved, they are
added to the context considered for later passages.</para>

<para>The input to this query is an ordered sequence of
<emphasis>passages</emphasis> which have been independently analyzed to
extract proper names.  The query uses three particular relationships
in the disambiguation process:
<itemizedlist>

<listitem><para><literal>@?gn/proper-names</literal> contains the proper names
heuristically extracted from a passage.</para></listitem>

<listitem><para><literal>@?part-of*</literal> is the recursive closure
of the <literal>part-of</literal> relationship which, among other
roles, is used to describe geographic containment.</para></listitem>

<listitem><para><literal>@?isa</literal> relates individuals to the
concepts that can describe them.</para></listitem>

</itemizedlist>
</para>

<para>Finally, the query logic uses a number of additional Scheme
operators:
<itemizedlist>

<listitem><para><literal>try</literal> executes a set of clauses
(sub-queries) in order, returning the first one to have any
results.</para></listitem>

<listitem><para><literal>doseq</literal> iterates over the elements of
an ordered sequence.</para></listitem>

<listitem><para><literal>singleton</literal> executes a query and
returns the result only if it is unique: i.e. the result set contains
exactly one item.</para></listitem>

<listitem><para><literal>get-basis</literal> takes a set of frames and
a slotid and enumerates the forest of frames generated by recursive
traversal, returning the subset of the original set which are roots of
trees in this forest.</para></listitem>

<listitem><para><literal>stdstring</literal> converts a name to a
canonical form (lowercase, consistently spaced, and
unicode-normalized) for the database.  search</para></listitem>

<listitem><para><literal>set+!</literal> extends a result set with
another result or result set.</para></listitem>

<listitem><para><literal>qc</literal> inhibits the natural iteration
over result sets done by procedure calls.  It is used to pass the
context around without invoking this automatic
iteration.</para></listitem>

</itemizedlist>
</para>

<para>The first phase of the query is implemented by this pair of procedures:
<programlisting>
(define (phase1-resolve-name name)
  ;; Get all possible candidates
  (let ((candidates (choice (?? 'names (stdstring name)))))
    (try (singleton candidates)
	 (singleton (get-basis candidates @?genls))
	 (singleton (get-basis candidates @?specls)))))
(define (phase1 node)
  ;; Get all the unambiguous references and the concepts associated with them.
  (let ((names (phase1-resolve-name (get node @?gn/proper-names))))
    (choice names (get names @?isa))))
</programlisting>

<literal>SINGLETON</literal> is used to ensure that only unambiguous
referents are returned, while <literal>GET-BASIS</literal> is used to
handle the common case where the same individual may have different
representations for different roles.  (For instance, nodes for Arnold
Schwarzenegger as an actor, governor, and person).  The combination of
these procedures (requiring uniqueness of the individual but allowing
multiple descriptors of the same individual) is used as the context
for the second phase.</para>

<para>The second phase of the query logic uses contextual information
to further constrain the search.  Note that it uses this contextual
information to attempt to disambiguate by both location and kind.
<programlisting>
(define (phase2-resolve-name name context)
  (try (intersection (phase1-resolve-name name) context)
    (let ((candidates (choice (?? 'names (stdstring name) @?part-of* context)
	 		    (?? 'names (stdstring name) @?isa context))))
      (try (singleton candidates)
	   (singleton (get-basis candidates @?genls))
           (singleton (get-basis candidates @?specls))))))
(define (phase2 node context)
  (phase2-resolve-name (get node @?gn/proper-names) (qc context)))
</programlisting>
</para>

<para>The phases are combined very simply:
<programlisting>
(define (getrefs passages)
  (let ((context {}))
    (doseq (passage passages) (set+! context (phase1 passage)))
    (doseq (passage passages)
      (set+! context (phase2 passage (qc context)))) context))
</programlisting>
</para>

</sect2>

<sect2><title>Task Performance</title>

<para>We executed 200 trials of the query using sets of passages
derived from an analyzed database of articles from various magazines
provided by Time, Inc.  The average execution time for queries without
the method was 1.85 seconds and 1.32 seconds with the method.  The
number of passages, as well as their size and number of proper names,
varied substantially between samples.  Queries made an average of
6,578 database references and the query logic itself took an average
of 15 milliseconds.  Object fetch time with the method averaged 1.83
seconds without the method and 1.14 seconds with the method.</para>

<para>On this task, the average improvement was 80%, rising in some
cases to nearly 500%.  However, due to the complexity of the query
logic and the variation in the input cases, the behavior is much less
consistently tied to the various task parameters.</para>

<figure><title>Comparison on &getrefsTASK; against database references</title>
 <mediaobject>
  <imageobject>
    <imagedata fileref="getrefs/dbrefs.png" format="PNG" scale="33"/>
  </imageobject>
  <imageobject>
   <imagedata fileref="getrefs/dbrefs" format="EPS" scale="33"/>
  </imageobject>
 </mediaobject>
</figure>

<figure><title>Comparison on &getrefsTASK; against the query logic time</title>
 <mediaobject>
  <imageobject>
    <imagedata fileref="getrefs/exectime.png" format="PNG" scale="33"/>
  </imageobject>
  <imageobject>
   <imagedata fileref="getrefs/exectime" format="EPS" scale="33"/>
  </imageobject>
 </mediaobject>
</figure>

<figure><title>Comparison on &getrefsTASK; against fetch time</title>
 <mediaobject>
  <imageobject>
    <imagedata fileref="getrefs/fetchtime.png" format="PNG" scale="33"/>
  </imageobject>
  <imageobject>
   <imagedata fileref="getrefs/fetchtime" format="EPS" scale="33"/>
  </imageobject>
 </mediaobject>
</figure>

</sect2>

<sect2><title>The Lifecycle of a particular query</title>

<para>The following diagram shows the progress of a particular query
in the system.  As before, time is shown along the horizontal axis.
However, number of database references in each fetch cycle for this
query ranges from 6 in the first cycle (retriving the passages
themselves) to 11,520 in the fourth cycle.  Consequently, we use a log
scale for the vertical axis indicating database references.
<figure><title>Execution timeline for a REFERENCE task</title>
 <mediaobject>
  <imageobject>
    <imagedata fileref="getrefs/timeline.png" format="PNG" scale="33"/>
  </imageobject>
  <imageobject>
   <imagedata fileref="getrefs/timeline" format="EPS" scale="33"/>
  </imageobject>
 </mediaobject>
</figure>
</para>
</sect2>

</sect1>

-->

<sect1 id='Achilles'><title>Applicability: The Achilles Heel of Semantic Web Services</title>

<para>The vision of the Semantic Web, and its more common cousin the
web services vision, is widely imagined to be the shape of the
information technology future.  A machine-usable web --- which both of
these visions have at their core --- is expected to do for
applications what the World Wide Web did for human information access
and lead to an explosion in capabilities and markets.</para>

<para>Unfortunately, both of these visions of a machine-usable web,
especially the Semanatic Web, have a widely unrecognized Achilles
Heel.  That Achilles Heel is <emphasis
role='strong'>latency</emphasis> which is, briefly, the time it takes
to get a simple answer across a network.  While bandwdith capacities
of networks have been growing by leaps and bounds, latency has only
improved slowly.</para>

<para>Latency is not that much of a problem for the human web, since
humans are relatively slow processors of information and normal
network latencies are practically imperceptible on a human scale.  For
computers, however, latency is more of a problem because processing is
so much faster (and getting faster) so that slow responses means that
they are spending more and more of their time idle.</para>

<para>Latency has two major causes, neither of which is likely to
diminish substantially.  The first problem is transmission delay,
which is firmly limited by the speed of light.  This may seem a
distant limit, but consider a client program executing a query on a
computer in Los Angeles which uses data from a server in Boston.  The
round trip of 6,000 miles, even if it went directly at the speed of
light (186,000 miles/second), would take roughly 32 milliseconds.
This is <literal role='strong'>two orders of magnitude</literal>
longer than the average execution time of the query logic for the
&hyponymsTASK; and &subclassesTASK; tasks and these tasks require (on average) 17
or 140 such references respectively.  Clearly there is a potential
problem.</para>

<para>The second cause of latency comes from network overhead and the
number and speed of switchers and routers along a typical path between
processing nodes.  This grows with the size and complexity of the
network and while it is reduced by faster computation, it still plays
a significant factor.  To use some real numbers, the latency between
the two nodes used in the benchmarks above (which were on a common
wired network running through a single switch) averaged 160
microseconds, while the latency from the query machine (based in
Boston) to a remote data center (based in Texas) averaged 25
milliseconds.</para>

<para>For both causes, parts of the problem can be addressed by local
replication or mirroring, moving copies of information closer (in
distance and network jumps) to the client.  This is already done for
human content by companies like Akamai and it is well understood.
However, it requires some significant up-front work and may not be
able to scale to the million-producer model of the Semantic Web
vision.</para>

<para>The method we describe here, iterated partial evaluation, helps
solve this problem in two ways.  First, it introduces the notion of
bundling transactions together to reduce the impact of latency costs.
And second, it uses iterated partial evaluation to extract such
bundles of transactions from complex procedural queries.  While other
methods, such as formal query analysis, could identify those same
bundles, they may be difficult to determine in general and may invoke
their own local performance issues.</para>

<para>To look at this concretely, we reran the above benchmarks
accessing a knowledge server running at the Texas-based data center
described above.  Because query execution took so long, we used a
single execution of each task for our timings, rather than the
"average of three" used above.</para>

</sect1>

<sect1 id='Conclusions'><title>Conclusions</title>

<para>We have demonstrated the performance advantages of iterated
partial evaluation for a variety of query tasks, illustrating both the
power and generality of the technique.  The four benchmark tasks
included simple recursive retrieval, recursive retrieval with
inference, and two moderately complex algorithms with substantial
internal state.  In the first two tasks, the method improved
performance by 114% and 88% respectively, with increasing gains (up to
nearly 900% and over 400% respectively) as the queries increased in
size and complexity.  In the third task, modest average gains of 15%
increased in some cases to nearly 600% and a simple rewriting of the
query logic resulted in an average speedup of over 200%.  In the final
task, the average performance increase averaged 120% and rose on some
tasks by over 700%.</para>

<para>Improvements in execution performance, through natural
improvements in processor technology or capability, will only further
enhance this advantage.  In addition, further aggressive engineering
of the functional core may also yield increases in execution times
which will yield further gains, though execution improvements alone
will eventually be subject to diminishing returns as database access
time increasingly dominates performance.  However, it may also be
possible to further improve aggregate performance by allowing the
overlap of execution and fetch cycles and providing for ways for
partial results to be cached across execution phases.  This latter
optimization is already partially implemented in the way that slot
accesses, as described above, cache their values when their
computation resolved completely without requiring any additional
data.</para>

<para>As we mentioned above, the method is especially well suited to
queries with complex internal dependencies which access multiple
networked data resources.  The first of these criteria describe many
inference and data analysis tasks, especially intelligent
knowledge-based processing.  The second criteria describes the
emerging execution context of network-based applications utilizing web
services or other distributed information sources.</para>

<para>The two criteria come together in the architecture for a
"Semantic Web" put forth by Tim Berners-Lee and others.  Widely
expected to be the foundation of the next "information revolution," it
makes complex inferences combining information from diverse
distributed information sources.  The methods described in this
article are ideally suited to this environment.</para>

</sect1>

<bibliography id="references"><title>Bibliography</title>

<biblioentry id='MinskyFrames'>
  <abbrev id="MinskyFrames.abbrev">Minsky 1977</abbrev>
  <biblioset relation='chapter'>
   <title>A Framework For Representing Knowledge</title>
   <author><surname>Minsky</surname><firstname>Marvin</firstname></author>
  </biblioset>
 <biblioset relation='book'>
  <title>The Psychology of Computer Vision</title>
  <editor><surname>Winston</surname><firstname>Patrick</firstname></editor>
 <publisher>
   <publishername>McGraw-Hill</publishername>
   <address>New York, NY</address>
 </publisher>
 <pubdate>1977</pubdate>
 </biblioset>
</biblioentry>
<biblioentry id='BRICO'>
 <abbrev id="BRICO.abbrev">Haase 2000</abbrev>
 <biblioset relation='article'>
  <title>Interlignual BRICO</title>
  <author><surname>Haase</surname><firstname>Kenneth</firstname></author>
 </biblioset>
 <biblioset relation='journal'>
  <title>IBM Systems Journal</title>
  <corpname>IBM</corpname>
  <pubdate>2000</pubdate>
  <volumenum>39</volumenum>
  <issuenum>3&amp;4</issuenum>
 </biblioset>
</biblioentry>
<biblioentry id='WordNet'>
 <abbrev id="WordNet.abbrev">Fellbaum 1998</abbrev>
 <title>WordNet: An Electronic Lexical Database</title>
 <editor><surname>Fellbaum</surname><firstname>Christiane</firstname></editor>
 <publisher>
   <publishername>MIT Press</publishername>
   <address>Cambridge, MA</address>
 </publisher>
 <pubdate>1998</pubdate>
</biblioentry>
<biblioentry id='BabelVision'>
 <abbrev id="BabelVision.abbrev">Haase 2004a</abbrev>
 <biblioset relation='article'>
  <title>BabelVision: Better Image Searching through Shared Annotations</title>
  <authorgroup>
   <author><surname>Haase</surname><firstname>Kenneth</firstname></author>
   <author><surname>Tams</surname><firstname>David</firstname></author>
  </authorgroup>
 </biblioset>
 <biblioset relation='journal'>
  <title>ACM Interactions</title>
  <date>February 2004</date>
 </biblioset>
</biblioentry>
<biblioentry id='FramerD'>
  <abbrev id="FramerD.abbrev">Haase 1996</abbrev>
  <biblioset relation='article'>
  <title>FramerD: Representing Knowledge in the Large</title>
  <author><surname>Haase</surname><firstname>Kenneth</firstname></author>
 </biblioset>
 <biblioset relation='journal'>
  <title>IBM Systems Journal</title>
  <corpname>IBM</corpname>
  <pubdate>1996</pubdate>
  <volumenum>35</volumenum>
  <issuenum>3&amp;4</issuenum>
 </biblioset>
</biblioentry>
<biblioentry id='ContextDisambig'>
 <abbrev id="ContextDisambig.abbrev">Haase 2004b</abbrev>
 <biblioset relation='article'>
  <title>Context for Semantic Metadata</title>
  <author><surname>Haase</surname><firstname>Kenneth</firstname></author> 
 </biblioset>
 <biblioset relation='conference'>
  <title>Proceedings of ACM Multimedia 2004</title>
  <publisher>
    <publishername>ACM Press</publishername>
  </publisher>
  <pubdate>2004</pubdate>
 </biblioset>
</biblioentry>
<biblioentry id='CYCL'>
  <abbrev id="CYCL.abbrev">Lenat &amp; Guha 1990</abbrev>
  <title>Building Large Knowledge Based Systems</title>
  <authorgroup>
   <author>
    <firstname>Douglas</firstname>
    <surname>Lenat</surname>
   </author>
   <author>
    <firstname>R.V.</firstname>
    <surname>Guha</surname>
   </author>
  </authorgroup>
  <publisher>
   <publishername>Addison Wesley</publishername>
   <address>New York, US</address>
  </publisher>
  <pubdate>1990</pubdate>
</biblioentry>

</bibliography>

</article>
